# Real-World Benchmarks and Use Cases

This document tracks benchmark workloads that represent how FUSE is expected to be used in practice.

Run the benchmark harness:

```bash
scripts/use_case_bench.sh
```

Run AOT performance harness (cold start + steady-state throughput):

```bash
scripts/aot_perf_bench.sh
```

Median-of-3 mode (used by reliability repeat CI for lower host-noise sensitivity):

```bash
scripts/use_case_bench.sh --median-of-3
```

Outputs:

- `.fuse/bench/use_case_metrics.md`
- `.fuse/bench/use_case_metrics.json`
- `.fuse/bench/aot_perf_metrics.md`
- `.fuse/bench/aot_perf_metrics.json`

Regression gate:

```bash
scripts/check_use_case_bench_regression.sh
```

AOT cold-start SLO gate:

```bash
scripts/check_aot_perf_slo.sh
```

Baseline + thresholds live in `benchmarks/use_case_baseline.json`.

## Workload matrix

| Use case | Target project | Why this workload exists | Metrics collected |
| --- | --- | --- | --- |
| CLI tool with config + validation | `examples/project_demo.fuse` | Covers env-backed config, refined types, and runtime contract failures in a command-style app | `check` latency, run latency (valid), run latency (contract failure path) |
| Non-toy backend API package | `examples/notes-api` | Covers package check/migrate/run flow with HTTP routes, type-checked request/response boundaries, and DB usage | cold/warm check latency, migrate latency, API request latencies (when loopback is available) |
| Frontend client integration | `examples/notes-api` (`GET /` + API calls) | Covers serving static UI alongside API boundary validation behavior in one service | root document latency, valid/invalid JSON request latency, validation-error overhead metric (when loopback is available) |
| AOT deployment readiness | `examples/project_demo.fuse` (temporary fixture package generated by harness) | Quantifies production startup and steady-state delta between `fuse run --backend native` and `fuse build --aot --release` output | cold-start completion distribution (`p50/p95/p99`) and CLI run-throughput distribution (`p50/p95/p99`) for JIT-native vs AOT |

## Metric definitions

- Compile/check times:
  - first `check`: cold package semantic compile/check path
  - second `check`: warm package semantic compile/check path
- Boundary check times:
  - `POST /api/notes` with valid and invalid JSON bodies
  - invalid payload is expected to fail with `400`
- Validation error overhead:
  - absolute latency delta between invalid and valid `POST /api/notes`
  - emitted as `request_validation_error_overhead_abs_ms`
  - used as a practical signal for validation/error-mapping overhead
- AOT cold-start distribution:
  - measured as process start to successful CLI completion (`project_demo` run)
  - emitted for JIT-native and AOT as `p50/p95/p99`
  - improvement percentages emitted as `cold_start_ms.improvement_pct`
- Steady-state throughput:
  - measured as sequential CLI runs per second after one warmup run
  - emitted for JIT-native and AOT as `p50/p95/p99`
  - improvement percentages emitted as `steady_state_cli_runs_per_sec.improvement_pct`

## Notes

- Results are environment-dependent; compare trends across runs on the same machine.
- The harness intentionally checks status codes so failures are semantic regressions, not only performance noise.
- Runtime HTTP metrics are required for `scripts/use_case_bench.sh`; that harness fails if the service cannot be started or reached on loopback.
- `scripts/release_smoke.sh` retries benchmark collection/check once if the first regression check fails, to filter transient host jitter without changing baseline thresholds.
- `scripts/reliability_repeat.sh` uses `--median-of-3` when collecting benchmark metrics.
- `scripts/reliability_repeat.sh` also runs `scripts/aot_perf_bench.sh` and `scripts/check_aot_perf_slo.sh`.
- CI uploads benchmark artifacts (`.fuse/bench/*.json` + `.fuse/bench/*.md`) for trend tracking across runs.
- This benchmark set is complementary to semantic parity gates (`scripts/semantic_suite.sh`, `scripts/authority_parity.sh`).

## Baseline refresh policy

`benchmarks/use_case_baseline.json` is a contract file and should only be refreshed when:

- performance changes are intentional and persistent, or
- harness methodology changed (for example, metric definition or collection mode changes).

Required process for baseline refreshes:

1. Obtain explicit maintainer approval in the PR before updating the baseline values.
2. Run `scripts/use_case_bench.sh` (or `scripts/use_case_bench.sh --median-of-3` if that is the target mode) on the same host class used for comparison.
3. Update baseline metric values and metadata:
   - `generated_utc` must be a fresh ISO-8601 UTC timestamp.
   - `refresh_rationale` must explain why the refresh is required.
4. Run `scripts/check_use_case_bench_regression.sh` to validate format + thresholds after the update.

`scripts/check_use_case_bench_regression.sh` enforces the required metadata fields (`generated_utc`, `refresh_rationale`) so baseline updates are auditable.
